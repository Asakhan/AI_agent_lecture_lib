ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ ì •ë¦¬ ê¸°ëŠ¥ê³¼ ëŒ€ì‹œë³´ë“œ ì¶œë ¥ ê¸°ëŠ¥ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.

[1. cleanup_old_memories ë©”ì„œë“œ]
def cleanup_old_memories(
    self, 
    days_old: int = 30,
    keep_important: bool = True
) -> Dict[str, int]:
    """
    ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        days_old: ì´ ì¼ìˆ˜ë³´ë‹¤ ì˜¤ë˜ëœ ë¬¸ì„œ ì‚­ì œ
        keep_important: ì¤‘ìš” ë¬¸ì„œ ë³´ì¡´ ì—¬ë¶€
    
    Returns:
        {"deleted": int, "kept": int}
    """
    
    from datetime import datetime, timedelta
    
    # ê¸°ì¤€ ë‚ ì§œ ê³„ì‚°
    cutoff_date = (datetime.now() - timedelta(days=days_old)).isoformat()
    
    # ì „ì²´ ë¬¸ì„œ ì¡°íšŒ
    all_docs = self.get_all_documents()
    
    deleted = 0
    kept = 0
    
    for doc in all_docs:
        timestamp = doc['metadata'].get('timestamp', '')
        
        # ì˜¤ë˜ëœ ë¬¸ì„œì¸ì§€ í™•ì¸
        if timestamp < cutoff_date:
            # ì¤‘ìš” ë¬¸ì„œ ì²´í¬
            if keep_important and doc['metadata'].get('important', False):
                kept += 1
                continue
            
            # ì‚­ì œ
            if self.delete_memory(doc['id']):
                deleted += 1
        else:
            kept += 1
    
    logger.info(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {deleted}ê°œ ì‚­ì œ, {kept}ê°œ ë³´ì¡´")
    
    return {
        "deleted": deleted,
        "kept": kept,
        "cutoff_date": cutoff_date
    }


[2. mark_as_important ë©”ì„œë“œ]
def mark_as_important(self, document_id: str) -> bool:
    """ë¬¸ì„œë¥¼ ì¤‘ìš”ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""
    
    try:
        # ë¬¸ì„œ ì¡°íšŒ
        doc = self.collection.get(ids=[document_id])
        
        if not doc['ids']:
            return False
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        metadata = doc['metadatas'][0]
        metadata['important'] = True
        
        # ë¬¸ì„œ ì—…ë°ì´íŠ¸ (ì‚­ì œ í›„ ì¬ìƒì„±)
        self.collection.delete(ids=[document_id])
        self.collection.add(
            ids=[document_id],
            documents=[doc['documents'][0]],
            metadatas=[metadata]
        )
        
        logger.info(f"ì¤‘ìš” ë¬¸ì„œ í‘œì‹œ: {document_id}")
        return True
        
    except Exception as e:
        logger.error(f"ì¤‘ìš” í‘œì‹œ ì‹¤íŒ¨: {e}")
        return False


[3. get_memory_stats ë©”ì„œë“œ]
def get_memory_stats(self) -> Dict[str, Any]:
    """ìƒì„¸í•œ ë©”ëª¨ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    all_docs = self.get_all_documents()
    
    # ì†ŒìŠ¤ë³„ ì¹´ìš´íŠ¸
    sources = {}
    for doc in all_docs:
        source = doc['metadata'].get('source', 'unknown')
        sources[source] = sources.get(source, 0) + 1
    
    # ë‚ ì§œë³„ ì¹´ìš´íŠ¸
    from datetime import datetime, timedelta
    now = datetime.now()
    
    date_counts = {
        'last_24h': 0,
        'last_7days': 0,
        'last_30days': 0,
        'older': 0
    }
    
    for doc in all_docs:
        timestamp_str = doc['metadata'].get('timestamp', '')
        if timestamp_str:
            timestamp = datetime.fromisoformat(timestamp_str)
            age = (now - timestamp).days
            
            if age < 1:
                date_counts['last_24h'] += 1
            if age < 7:
                date_counts['last_7days'] += 1
            if age < 30:
                date_counts['last_30days'] += 1
            else:
                date_counts['older'] += 1
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„
    text_lengths = [len(doc['text']) for doc in all_docs]
    avg_length = sum(text_lengths) / len(text_lengths) if text_lengths else 0
    
    return {
        'total_documents': len(all_docs),
        'collection_name': self.collection_name,
        'by_source': sources,
        'by_date': date_counts,
        'avg_text_length': int(avg_length),
        'embedding_dimension': self.embeddings.get_dimension(),
        'cache_info': self.embeddings.get_cache_info()
    }


[4. print_memory_dashboard ë©”ì„œë“œ]
def print_memory_dashboard(self):
    """ë©”ëª¨ë¦¬ í†µê³„ë¥¼ ëŒ€ì‹œë³´ë“œ í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    
    stats = self.get_memory_stats()
    
    print("=" * 60)
    print("ğŸ“Š ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ")
    print("=" * 60)
    print(f"ì»¬ë ‰ì…˜: {stats['collection_name']}")
    print(f"ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']:,}ê°œ")
    print(f"í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {stats['avg_text_length']:,}ì")
    print(f"ì„ë² ë”© ì°¨ì›: {stats['embedding_dimension']}")
    print()
    print("ğŸ“ ì†ŒìŠ¤ë³„ ë¶„í¬:")
    for source, count in stats['by_source'].items():
        print(f"  â€¢ {source}: {count}ê°œ")
    print()
    print("ğŸ“… ê¸°ê°„ë³„ ë¶„í¬:")
    print(f"  â€¢ ìµœê·¼ 24ì‹œê°„: {stats['by_date']['last_24h']}ê°œ")
    print(f"  â€¢ ìµœê·¼ 7ì¼: {stats['by_date']['last_7days']}ê°œ")
    print(f"  â€¢ ìµœê·¼ 30ì¼: {stats['by_date']['last_30days']}ê°œ")
    print(f"  â€¢ ê·¸ ì´ì „: {stats['by_date']['older']}ê°œ")
    print()
    print("ğŸ’¾ ìºì‹œ ì •ë³´:")
    cache_info = stats['cache_info']
    print(f"  â€¢ ìºì‹œ í¬ê¸°: {cache_info.get('size', 0)}ê°œ")
    print(f"  â€¢ ì ì¤‘ë¥ : {cache_info.get('hit_rate', 0):.1%}")
    print("=" * 60)


[5. EmbeddingGeneratorì— get_cache_info ì¶”ê°€]
src/utils/embeddings.pyì˜ EmbeddingGenerator í´ë˜ìŠ¤ì—:

def get_cache_info(self) -> Dict[str, Any]:
    """ìºì‹œ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        'size': len(self.cache),
        'hit_rate': self.cache_hits / max(self.total_requests, 1) if hasattr(self, 'cache_hits') else 0
    }

__init__ì— ì¹´ìš´í„° ì¶”ê°€:
self.cache_hits = 0
self.total_requests = 0

create_embedding ë©”ì„œë“œì—ì„œ:
self.total_requests += 1
if text in self.cache:
    self.cache_hits += 1


[í…ŒìŠ¤íŠ¸ ì½”ë“œ (ì£¼ì„)]
# # ì¤‘ìš” ë¬¸ì„œ í‘œì‹œ
# mm.mark_as_important(doc_id)
# 
# # 30ì¼ ì´ìƒ ì˜¤ë˜ëœ ë¬¸ì„œ ì •ë¦¬
# result = mm.cleanup_old_memories(days_old=30)
# print(f"âœ“ ì‚­ì œ: {result['deleted']}ê°œ, ë³´ì¡´: {result['kept']}ê°œ")
#
# # ëŒ€ì‹œë³´ë“œ ì¶œë ¥
# mm.print_memory_dashboard()